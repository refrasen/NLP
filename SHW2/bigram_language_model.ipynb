{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3: Language Modeling\n",
    "In this homework, you will be implementing a bigram language model on a dataset containing news headlines. Specifically, you will be tasked to complete the `count_bigrams`, `get_probability`, `sample_word`, and `calculate_perplexity` functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Here we will be importing the necessary modules, as well as doing some basic preprocessing of the text. You do not need to modify any code here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Reading in the training and development datasets\n",
    "with open(\"headlines.train\", 'r') as f:\n",
    "    headlines_train = f.readlines()\n",
    "with open(\"headlines.dev\", 'r') as f:\n",
    "    headlines_dev = f.readlines()\n",
    "    \n",
    "# Removing excess punctuation and newline\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "headlines_train = [regex.sub('', h.split(\"\\n\")[0]) for h in headlines_train]\n",
    "headlines_dev = [regex.sub('', h.split(\"\\n\")[0]) for h in headlines_dev]\n",
    "\n",
    "# Defining UNK, START and STOP tokens\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "START_TOKEN = \"<START>\"\n",
    "STOP_TOKEN = \"<STOP>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin, let's look at what some of the headlines look like. Run the following code block as many times as you want to get a sense of what kind of headlines we hope to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vcat to hear police station push\n\nex mayor laments loss of council experience\n\npinpointing the source of sea level rise\n\nnew shire administrator seeks lightning ridge\n\nashes highlights day three\n\n"
     ]
    }
   ],
   "source": [
    "for headline in random.sample(headlines_train, 5):\n",
    "    print(headline)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.a) Counting Bigrams\n",
    "Below we have `count_unigrams` implemented for you. Implement `count_bigrams`, which takes in a headline as text, and updates the given bigram dictionary with the bigram counts for that text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_unigrams(text, unigram_dict):\n",
    "    \"\"\"\n",
    "    :param text: A headline, consisting of a string of words\n",
    "    :param unigram_dict: A dictionary containing unigrams as keys and their respective counts as values\n",
    "    \"\"\"\n",
    "    tokens = [START_TOKEN] + text.split(\" \") + [STOP_TOKEN]\n",
    "    for i in range(len(tokens)):\n",
    "        unigram = tokens[i]\n",
    "        if unigram not in unigram_dict:\n",
    "            unigram_dict[unigram] = 1\n",
    "        else:\n",
    "            unigram_dict[unigram] += 1\n",
    "\n",
    "def count_bigrams(text, bigram_dict):\n",
    "    \"\"\"\n",
    "    :param text: A headline, consisting of a string of words\n",
    "    :param bigram_dict: A dictionary containing bigrams as keys and their respective counts as values\n",
    "    \"\"\"\n",
    "    tokens = [START_TOKEN] + text.split() + [STOP_TOKEN]\n",
    "\n",
    "    for i in range(len(tokens) - 1):\n",
    "        first_word = tokens[i]\n",
    "        second_word = tokens[i + 1]\n",
    "        bigram = first_word + \" \" + second_word\n",
    "        bigram_dict[bigram] = bigram_dict.get(bigram, 0) + 1\n",
    "    return bigram_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.b) Computing Probability\n",
    "Implement `get_probability`, which calculates the probability of seeing a word given the previous word, along with Laplace smoothing parameterized by `alpha`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_probability(target, context, unigram_dict, bigram_dict, alpha, vocab_size):\n",
    "    \"\"\"\n",
    "    :param target: The word whose probability of seeing is being computed given the context\n",
    "    :param context: The word directly preceeding the target word\n",
    "    :param unigram_dict: A dictionary containing unigrams as keys and their respective counts as values\n",
    "    :param bigram_dict: A dictionary containing bigrams as keys and their respective counts as values\n",
    "    :param alpha: The amount of additive smoothing being applied\n",
    "    :param vocab_size: The size of the training vocabulary\n",
    "    :return: The probability of seeing the target word given the context\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    counts_context = unigram_dict.get(context,0)\n",
    "    bigram = context + \" \" + target\n",
    "    counts_context_target = bigram_dict.get(bigram, 0)\n",
    "    prob = (counts_context_target + alpha) / (counts_context + vocab_size * alpha)\n",
    "    return prob \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.c) Word Sampling\n",
    "Once we can calculate the desired probabilities, we can now use that to sample words for generation. Implement `sample_word`, which samples a new word in accordance with its probability of following the previous word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_word(words, probs):\n",
    "    \"\"\"\n",
    "    :param words: The list of words to sample from\n",
    "    :param probs: The probabilities of seeing each word; probs[i] is the probability of seeing word[i]\n",
    "    :return: A word whose sampling likelihood is the probability of being seen given the context\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    r = random.random()\n",
    "    CDF = 0\n",
    "    x = 0\n",
    "    \n",
    "    for i in range(len(probs)):\n",
    "        CDF = CDF + probs[i]\n",
    "        x = i\n",
    "        if CDF >= r:\n",
    "            break\n",
    "    return words[x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating New Headlines\n",
    "Now that we've all the key parts of our language model completed, let's see how well we can generate new headlines! Run the following code block to complete the algorithm, and the subsequent code block as many times as you want to see what kind of new headlines we are able to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha = .000001 # Change this to see different levels of smoothing affect generated text\n",
    "min_freq = 10 # The minimum word frequency to be present in the vocabulary\n",
    "\n",
    "# The following are used to keep track of and remove infrequent words\n",
    "low_freq = set()\n",
    "all_words = {}\n",
    "\n",
    "def generate_headline(unigram_dict, bigram_dict, alpha):\n",
    "    sent = [START_TOKEN]\n",
    "    while not sent[-1] == STOP_TOKEN:\n",
    "        words = list(vocab)\n",
    "        probs = [get_probability(word, sent[-1], unigram_dict, bigram_dict, alpha, len(vocab)) for word in words]\n",
    "        next_word = sample_word(words, probs)\n",
    "        sent.append(next_word)\n",
    "    print(' '.join(sent[1:-1]))\n",
    "\n",
    "def replace_text_train(text):\n",
    "    return \" \".join([UNK_TOKEN if t in low_freq else t for t in text.split()])\n",
    "\n",
    "def replace_text_dev(text):\n",
    "    return \" \".join([UNK_TOKEN if t not in vocab else t for t in text.split()])\n",
    "\n",
    "# Finding all words with low frequency\n",
    "for h in headlines_train:\n",
    "    count_unigrams(h, all_words)\n",
    "for word, count in all_words.items():\n",
    "    if count <= min_freq:\n",
    "        low_freq.add(word)\n",
    "# Replacing low frequency words from training dataset with UNK\n",
    "headlines_train_clean = [replace_text_train(h) for h in headlines_train]\n",
    "\n",
    "# Initialize unigram and bigram dictionaries\n",
    "unigrams = {}\n",
    "bigrams = {}\n",
    "\n",
    "# Generating unigram and bigram dictionaries\n",
    "for h in headlines_train_clean:\n",
    "    count_unigrams(h, unigrams)\n",
    "    count_bigrams(h, bigrams)\n",
    "    \n",
    "# Creating the training vocabulary\n",
    "vocab = set([item for sublist in map(lambda x: x.split(\" \"), headlines_train_clean) for item in sublist])\n",
    "vocab.add(START_TOKEN)\n",
    "vocab.add(STOP_TOKEN)\n",
    "\n",
    "# Replacing unseen vocabulary from development dataset with UNK\n",
    "headlines_dev_clean = [replace_text_dev(h) for h in headlines_dev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sri lanka says he was killed in <UNK> to <UNK> as police out as first time\n\npocket <UNK> rate\n\npowerline funding cut for tiger airways plane plan\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "operation begins voting good progress says\n\n<UNK>\n\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    generate_headline(unigrams, bigrams, alpha)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.a) Calculating Perplexity\n",
    "Once we're able to generate new headlines, we can see how well our algorithm works when encountering text from an unseen development set. Implement `calculate_perplexity` below and run the code block below that to see what the perplexity of our algorithm is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_perplexity(text, unigram_dict, bigram_dict, alpha, vocab_size):\n",
    "    \"\"\"\n",
    "    :param text: A headline, consisting of a string of words\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    count_unigrams(text, unigram_dict)\n",
    "    count_bigrams(text, bigram_dict)\n",
    "    words = text.split()\n",
    "    sum_probs = 0\n",
    "    for i in range(len(words)-1):\n",
    "        sum_probs += math.log(get_probability(words[i+1], words[i], unigram_dict, bigram_dict, alpha, vocab_size))\n",
    "    return math.exp(-sum_probs/len(words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average perplexity on dev set: 54.89\n"
     ]
    }
   ],
   "source": [
    "perplexities = []\n",
    "for h in headlines_dev_clean:\n",
    "    perp = calculate_perplexity(h, unigrams, bigrams, alpha, len(vocab))\n",
    "    perplexities.append(perp)\n",
    "print(\"Average perplexity on dev set: %.02f\" % (sum(perplexities) / len(perplexities)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.b) Parameter Experimentation\n",
    "Play around with modifying the smoothing parameter `alpha`. How does changing the values of that parameter affect the generated headlines, as well as the perplexity on the development set? Write two such observations in the cell below. Find a value for `alpha` that gives the lowest perplexities; you should be able to get a perplexity below 1000. What value of `alpha` gets the optimal perplexity value?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lowering alpha usually brought the average perplexity down. I raised alpha a few magnitudes for the first time and headlines were sometimes a paragraph long and incoherent.\n",
    "Seems like the lowest I can get is 54.88 with $10^{-6}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) RNN Language Model\n",
    "List the three headlines you generated from running the rnn_language_model.ipynb notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mp mp foreign investment boost for states coast\n",
    "\n",
    "livestock plane creek restoration\n",
    "\n",
    "plane to make excites with nets it aussie chopper"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
